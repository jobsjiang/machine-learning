{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1、线性回归\n",
    "\n",
    "线性回归拟合一个带系数的线性模型，以最小化数据中的观测值与线性预测值之间的残差平方和。\n",
    "\n",
    "sklearn中也存在线性回归的算法库的接口，代码示例如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-06T16:19:54.731194Z",
     "start_time": "2019-05-06T16:19:53.976472Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "# 创建线性回归模型的对象\n",
    "regr = linear_model.LinearRegression()\n",
    "# 利用训练集训练线性模型\n",
    "regr.fit(X_train,y_train)\n",
    "# 使用测试集做预测\n",
    "y_pred = regr.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2、岭回归\n",
    "\n",
    "上述的线性回归算法使用最小二乘法优化各个系数，对于岭回归来说，岭回归通过对系数进行惩罚(L2范式)来解决普通最小二乘法的一些问题，例如，当特征之间完全共线性(有解)或者说特征之间高度相关，这个时候适合用岭回归。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#加载线性模型算法库\n",
    "from sklearn.linear_model import Ridge\n",
    "# 创建岭回归模型的对象\n",
    "reg = Ridge(alpha=.5)\n",
    "# 利用训练集训练岭回归模型\n",
    "reg.fit([[0, 0], [0, 0], [1, 1]], [0, .1, 1]) \n",
    "#输出各个系数\n",
    "reg.coef_\n",
    "reg.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3、Lasso回归\n",
    "\n",
    "Lasso是一个估计稀疏稀疏的线性模型。它在某些情况下很有用，由于它倾向于选择参数值较少的解，有效地减少了给定解所依赖的变量的数量。Lasso模型在最小二乘法的基础上加入L1范式作为惩罚项。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#加载Lasso模型算法库\n",
    "from sklearn.linear_model import Lasso\n",
    "# 创建Lasso回归模型的对象\n",
    "reg = Lasso(alpha=0.1)\n",
    "# 利用训练集训练Lasso回归模型\n",
    "reg.fit([[0, 0], [1, 1]], [0, 1])\n",
    "\"\"\"\n",
    "Lasso(alpha=0.1, copy_X=True, fit_intercept=True, max_iter=1000,\n",
    "normalize=False, positive=False, precompute=False, random_state=None,\n",
    "selection='cyclic', tol=0.0001, warm_start=False)\n",
    "\"\"\"\n",
    "# 使用测试集做预测\n",
    "reg.predict([[1, 1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4、Elastic Net回归\n",
    "\n",
    "Elastic Net 是一个线性模型利用L1范式和L2范式共同作为惩罚项。这种组合既可以学习稀疏模型，同时可以保持岭回归的正则化属性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#加载ElasticNet模型算法库\n",
    "from sklearn.linear_model import ElasticNet\n",
    "#加载数据集\n",
    "from sklearn.datasets import make_regression\n",
    "X, y = make_regression(n_features=2, random_state=0)\n",
    "#创建ElasticNet回归模型的对象\n",
    "regr = ElasticNet(random_state=0)\n",
    "# 利用训练集训练ElasticNet回归模型\n",
    "regr.fit(X, y)\n",
    "print(regr.coef_) \n",
    "print(regr.intercept_) \n",
    "print(regr.predict([[0, 0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5、贝叶斯岭回归\n",
    "\n",
    "贝叶斯岭回归模型和岭回归类似。贝叶斯岭回归通过最大化边际对数似然来估计参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import BayesianRidge\n",
    "X = [[0., 0.], [1., 1.], [2., 2.], [3., 3.]]\n",
    "Y = [0., 1., 2., 3.]\n",
    "reg = BayesianRidge()\n",
    "reg.fit(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6、SGD回归\n",
    "\n",
    "上述的线性模型通过最小二乘法来优化损失函数，SGD回归也是一种线性回归，不同的是，它通过随机梯度下降最小化正则化经验损失。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "n_samples, n_features = 10, 5\n",
    "np.random.seed(0)\n",
    "y = np.random.randn(n_samples)\n",
    "X = np.random.randn(n_samples, n_features)\n",
    "clf = linear_model.SGDRegressor(max_iter=1000, tol=1e-3)\n",
    "clf.fit(X, y)\n",
    "\"\"\"\n",
    "SGDRegressor(alpha=0.0001, average=False, early_stopping=False,\n",
    "epsilon=0.1, eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
    "learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
    "n_iter=None, n_iter_no_change=5, penalty='l2', power_t=0.25,\n",
    "random_state=None, shuffle=True, tol=0.001, validation_fraction=0.1,\n",
    "verbose=0, warm_start=False)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7、SVR\n",
    "\n",
    "众所周知，支持向量机在分类领域应用非常广泛，支持向量机的分类方法可以被推广到解决回归问题，这个就称为支持向量回归。支持向量回归算法生成的模型同样地只依赖训练数据集中的一个子集(和支持向量分类算法类似)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#加载SVR模型算法库\n",
    "from sklearn.svm import SVR\n",
    "#训练集\n",
    "X = [[0, 0], [2, 2]]\n",
    "y = [0.5, 2.5]\n",
    "#创建SVR回归模型的对象\n",
    "clf = SVR()\n",
    "# 利用训练集训练SVR回归模型\n",
    "clf.fit(X, y) \n",
    "\"\"\"\n",
    "SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1,\n",
    "gamma='auto_deprecated', kernel='rbf', max_iter=-1, shrinking=True,\n",
    "tol=0.001, verbose=False)\n",
    "\"\"\"\n",
    "clf.predict([[1, 1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8、KNN回归\n",
    "\n",
    "在数据标签是连续变量而不是离散变量的情况下，可以使用KNN回归。分配给查询点的标签是根据其最近邻居标签的平均值计算的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [[0], [1], [2], [3]]\n",
    "y = [0, 0, 1, 1]\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "neigh = KNeighborsRegressor(n_neighbors=2)\n",
    "neigh.fit(X, y) \n",
    "print(neigh.predict([[1.5]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9、决策树回归\n",
    "\n",
    "决策树也可以应用于回归问题，使用sklearn的DecisionTreeRegressor类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor \n",
    "X = [[0, 0], [2, 2]]\n",
    "y = [0.5, 2.5]\n",
    "clf = DecisionTreeRegressor()\n",
    "clf = clf.fit(X, y)\n",
    "clf.predict([[1, 1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10、神经网络\n",
    "\n",
    "神经网络使用slearn中MLPRegressor类实现了一个多层感知器(MLP)，它使用在输出层中没有激活函数的反向传播进行训练，也可以将衡等函数视为激活函数。因此，它使用平方误差作为损失函数，输出是一组连续的值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "mlp=MLPRegressor()\n",
    "mlp.fit(X_train,y_train)\n",
    "\"\"\"\n",
    "MLPRegressor(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
    "beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
    "hidden_layer_sizes=(100,), learning_rate='constant',\n",
    "learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
    "n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
    "random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
    "validation_fraction=0.1, verbose=False, warm_start=False)\n",
    "\"\"\"\n",
    "y_pred = mlp.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11.RandomForest回归\n",
    "\n",
    "RamdomForest回归也是一种经典的集成算法之一。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "X, y = make_regression(n_features=4, n_informative=2,\n",
    "random_state=0, shuffle=False)\n",
    "regr = RandomForestRegressor(max_depth=2, random_state=0,\n",
    "n_estimators=100)\n",
    "regr.fit(X, y)\n",
    "print(regr.feature_importances_)\n",
    "print(regr.predict([[0, 0, 0, 0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11、XGBoost回归\n",
    "\n",
    "XGBoost近些年在学术界取得的成果连连捷报，基本所有的机器学习比赛的冠军方案都使用了XGBoost算法，对于XGBoost的算法接口有两种，这里我仅介绍XGBoost的sklearn接口。更多请参考： \n",
    "\n",
    "https://xgboost.readthedocs.io/en/latest/python/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "xgb_model = xgb.XGBRegressor(max_depth = 3,\n",
    "learning_rate = 0.1,\n",
    "n_estimators = 100,\n",
    "objective = 'reg:linear',\n",
    "n_jobs = -1)\n",
    "xgb_model.fit(X_train, y_train,\n",
    "eval_set=[(X_train, y_train)], \n",
    "eval_metric='logloss',\n",
    "verbose=100)\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "print(mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12、LightGBM回归\n",
    "\n",
    "LightGBM作为另一个使用基于树的学习算法的梯度增强框架。在算法竞赛也是每逢必用的神器，且要想在竞赛取得好成绩，LightGBM是一个不可或缺的神器。相比于XGBoost，LightGBM有如下优点，训练速度更快，效率更高效；低内存的使用量。对于LightGBM的算法接口有两种，这里我同样介绍LightGBM的sklearn接口。更多请参考：https://lightgbm.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "gbm = lgb.LGBMRegressor(num_leaves=31,\n",
    "learning_rate=0.05,\n",
    "n_estimators=20)\n",
    "gbm.fit(X_train, y_train,\n",
    "eval_set=[(X_train, y_train)], \n",
    "eval_metric='logloss',\n",
    "verbose=100)\n",
    "y_pred = gbm.predict(X_test)\n",
    "print(mean_squared_error(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
