{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "https://mp.weixin.qq.com/s?__biz=MzI1MzY0MzE4Mg==&mid=2247485721&idx=2&sn=6665df59312644ed446b67d2aea0646b&chksm=e9d019d8dea790ce8a9f5d4fef8dfaf01dee86460193668c1a8c099b98058125272102d81f77&scene=0&xtrack=1&key=4c932049eeeeca5c5527eea8d83dbcb8ef2b836ad274a856c2c93a90a3d9da661e6b3526bb530e16af1896218a3d897a3a7eb8fd0d89e87e2dc017d63343dfbf97a07a754262689f1ca0272d2723fb63&ascene=1&uin=MjA1MjAyODkxNg%3D%3D&devicetype=Windows+10&version=62070158&lang=zh_CN&exportkey=Ab5UPYWRhVlB7NGC4wFNvCA%3D&pass_ticket=dOr5snmZeZWjJTijMUKH9LfQIuuqK3YNNKeS3poHXSUNendFXSAfrKHr5rdsm3m%2B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>兴趣</th>\n",
       "      <th>兴趣数量</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>健身 电影 音乐</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>电影 音乐</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>电影 篮球</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>篮球 羽毛球</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         兴趣  兴趣数量\n",
       "0  健身 电影 音乐     3\n",
       "1     电影 音乐     2\n",
       "2     电影 篮球     2\n",
       "3    篮球 羽毛球     2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 统计单词的个数作为特征\n",
    "import pandas as pd\n",
    "\n",
    "# 构造数据集\n",
    "df = pd.DataFrame({'兴趣': ['健身 电影 音乐', '电影 音乐', '电影 篮球', '篮球 羽毛球', ]})\n",
    "# 统计兴趣爱好的数量\n",
    "df['兴趣数量'] = df['兴趣'].apply(lambda x: len(x.split()))\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>edu_level</th>\n",
       "      <th>edu_level_map</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>博士</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>硕士</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>大学</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>大专及以下</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  edu_level  edu_level_map\n",
       "0        博士              4\n",
       "1        硕士              3\n",
       "2        大学              2\n",
       "3     大专及以下              1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 字典映射\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({'edu_level': ['博士', '硕士', '大学', '大专及以下']})\n",
    "#构建学历字典\n",
    "mapping_dict={'博士':4,'硕士':3,'大学':2,'大专及以下':1}\n",
    "#调用map方法进行转换\n",
    "df['edu_level_map']=df['edu_level'].map(mapping_dict)\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "类别：['程序员' '老师' '警察' '销售']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 0],\n",
       "       [1, 0, 0, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 0, 1],\n",
       "       [0, 0, 0, 1]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 标签二值化（LabelBinarizer）\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "df = pd.DataFrame({'职业': ['老师', '程序员', '警察', '销售', '销售', ]})\n",
    "\n",
    "lb = LabelBinarizer()\n",
    "lb.fit(df['职业'])\n",
    "print('类别：{}'.format(lb.classes_))\n",
    "# 输出：类别：['程序员' '老师' '警察' '销售']\n",
    "display(lb.fit_transform(df['职业']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>兴趣</th>\n",
       "      <th>兴趣列表</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>健身 电影 音乐</td>\n",
       "      <td>[健身, 电影, 音乐]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>电影 音乐</td>\n",
       "      <td>[电影, 音乐]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>电影 篮球</td>\n",
       "      <td>[电影, 篮球]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>篮球 羽毛球</td>\n",
       "      <td>[篮球, 羽毛球]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         兴趣          兴趣列表\n",
       "0  健身 电影 音乐  [健身, 电影, 音乐]\n",
       "1     电影 音乐      [电影, 音乐]\n",
       "2     电影 篮球      [电影, 篮球]\n",
       "3    篮球 羽毛球     [篮球, 羽毛球]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 多标签二值化（MultiLabelBinarizer）\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "# 构造数据集\n",
    "df = pd.DataFrame({'兴趣': ['健身 电影 音乐', '电影 音乐', '电影 篮球', '篮球 羽毛球', ]})\n",
    "df['兴趣列表'] = df['兴趣'].apply(lambda x: x.split())\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "类别顺序：['健身' '电影' '篮球' '羽毛球' '音乐']\n",
      "[[1 1 0 0 1]\n",
      " [0 1 0 0 1]\n",
      " [0 1 1 0 0]\n",
      " [0 0 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "# 构造特征\n",
    "mlb = MultiLabelBinarizer()\n",
    "feature_array = mlb.fit_transform(df['兴趣列表'])\n",
    "print('类别顺序：{}'.format(mlb.classes_))\n",
    "print(feature_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "shape=(4, 10)\n",
      "[[ 0.          0.          0.          0.          0.          0.\n",
      "  -0.57735027  0.57735027 -0.57735027  0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.81649658\n",
      "   0.          0.40824829 -0.40824829  0.        ]\n",
      " [ 0.          0.5         0.          0.         -0.5        -0.5\n",
      "   0.          0.         -0.5         0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "  -0.57735027  0.57735027 -0.57735027  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# 特征哈希（Feature Hashing）\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This is the second second document.',\n",
    "    'And the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "hv = HashingVectorizer(n_features=10)\n",
    "X = hv.fit_transform(corpus)\n",
    "print('shape={}'.format(X.shape))\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 1 0 0 1 0 1]\n",
      " [0 1 0 1 0 2 1 0 1]\n",
      " [1 0 0 0 1 0 1 1 0]\n",
      " [0 1 1 1 0 0 1 0 1]]\n",
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n"
     ]
    }
   ],
   "source": [
    "# 词袋模型(BOW) 使用范围：长文本特征\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This is the second second document.',\n",
    "    'And the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(X.toarray())\n",
    "feature_name = vectorizer.get_feature_names()\n",
    "print(feature_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.43877674 0.54197657 0.43877674 0.         0.\n",
      "  0.35872874 0.         0.43877674]\n",
      " [0.         0.27230147 0.         0.27230147 0.         0.85322574\n",
      "  0.22262429 0.         0.27230147]\n",
      " [0.55280532 0.         0.         0.         0.55280532 0.\n",
      "  0.28847675 0.55280532 0.        ]\n",
      " [0.         0.43877674 0.54197657 0.43877674 0.         0.\n",
      "  0.35872874 0.         0.43877674]]\n",
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF 使用范围：长文本特征\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# 构造数据集\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This is the second second document.',\n",
    "    'And the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "tfidf = TfidfVectorizer()\n",
    "X = tfidf.fit_transform(corpus)\n",
    "print(X.toarray())\n",
    "feature_name = tfidf.get_feature_names()\n",
    "print(feature_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    沙 瑞金 赞叹 易 学习 的 胸怀 ， 是 金山 的 百姓 有福 ， 可是 这件 事对 李达...\n",
      "1    沙 瑞金 向 毛娅 打听 他们 家 在 京州 的 别墅 ， 毛娅 笑 着 说 ， 王 大路 ...\n",
      "2    347 年 （ 永和 三年 ） 三月 ， 桓 温兵 至 彭模 （ 今 四川 彭山 东南 ） ...\n",
      "Name: 文章分词, dtype: object\n",
      "  (0, 42)\t1\n",
      "  (0, 71)\t1\n",
      "  (0, 18)\t1\n",
      "  (0, 54)\t1\n",
      "  (0, 34)\t1\n",
      "  (0, 92)\t1\n",
      "  (0, 73)\t1\n",
      "  (0, 29)\t1\n",
      "  (0, 63)\t1\n",
      "  (0, 11)\t1\n",
      "  (0, 2)\t2\n",
      "  (0, 43)\t2\n",
      "  (0, 78)\t2\n",
      "  (0, 82)\t1\n",
      "  (0, 87)\t1\n",
      "  (0, 7)\t2\n",
      "  (0, 36)\t5\n",
      "  (0, 25)\t1\n",
      "  (0, 24)\t1\n",
      "  (0, 89)\t1\n",
      "  (0, 91)\t1\n",
      "  (0, 80)\t1\n",
      "  (0, 31)\t1\n",
      "  (0, 1)\t2\n",
      "  (0, 35)\t2\n",
      "  :\t:\n",
      "  (2, 20)\t1\n",
      "  (2, 77)\t1\n",
      "  (2, 56)\t1\n",
      "  (2, 44)\t1\n",
      "  (2, 49)\t1\n",
      "  (2, 27)\t1\n",
      "  (2, 50)\t2\n",
      "  (2, 70)\t1\n",
      "  (2, 60)\t1\n",
      "  (2, 17)\t1\n",
      "  (2, 85)\t1\n",
      "  (2, 72)\t1\n",
      "  (2, 39)\t2\n",
      "  (2, 30)\t1\n",
      "  (2, 26)\t1\n",
      "  (2, 68)\t1\n",
      "  (2, 10)\t1\n",
      "  (2, 46)\t1\n",
      "  (2, 32)\t1\n",
      "  (2, 47)\t2\n",
      "  (2, 66)\t1\n",
      "  (2, 6)\t1\n",
      "  (2, 5)\t1\n",
      "  (2, 62)\t1\n",
      "  (2, 0)\t1\n",
      "[[0.011413   0.988587  ]\n",
      " [0.01987086 0.98012914]\n",
      " [0.02454516 0.97545484]]\n",
      "['347', '一起', '万块', '三人', '三套', '三年', '三月', '下海经商', '不想', '不要', '东南', '东挪西撮', '之后', '事业有成', '事对', '京州', '京州帝', '亲率', '以沫', '公司', '击退', '分开', '别墅', '前一晚', '县当', '县长', '参军', '同月', '名下', '后来', '周楚', '喝酒', '四川', '回忆起', '困难', '大家', '大路', '太大', '好像', '孙盛', '学习', '家住', '容易', '对不起', '将领', '康和易', '彭山', '彭模', '很大', '成汉', '成都', '战三胜', '房子', '打听', '时期', '有福', '李福', '李达', '李达康', '欧阳', '步兵', '毛娅', '永和', '没想到', '没有', '浪费', '温兵', '瑞金', '留下', '百姓', '直攻', '相助', '看守', '竟然', '而桓温', '股权', '胸怀', '袭击', '觉得', '触动', '话别', '豪园', '赔礼道歉', '赞叹', '踏实', '辎重', '这件', '连连', '逼近', '道口', '金山', '降职', '风生水']\n",
      "[[0.8881504  0.79494616 0.88179783 0.72261358 0.7825649  0.80513119\n",
      "  0.82520957 0.78049205 0.81985708 0.8075189  0.87151537 0.76026665\n",
      "  0.65725012 0.83192188 0.90323191 0.71051575 0.88162668 0.86249018\n",
      "  0.72348522 0.6859634  0.85216699 0.83801094 0.74827181 0.69732318\n",
      "  0.74391631 0.74718763 0.69555681 0.76728713 0.74016533 0.79252597\n",
      "  0.77958582 0.79331665 0.74595963 0.75151718 0.727614   0.68439365\n",
      "  0.7365373  0.82113611 0.83377043 0.80014547 0.81893092 0.7554505\n",
      "  0.75366236 0.77624709 0.71719464 0.84291399 0.80728078 0.70518523\n",
      "  0.83663851 0.76965302 0.76998538 0.84991333 0.82899283 0.83610283\n",
      "  0.77734802 0.79315431 0.71280155 0.85322984 0.75106508 0.76941605\n",
      "  0.89065397 0.87517217 0.76503638 0.81516416 0.82584699 0.85074834\n",
      "  0.79851313 0.73489237 0.79478729 0.84659611 0.75482478 0.87778172\n",
      "  0.73947408 0.79949669 0.74764634 0.82890922 0.79219997 0.7224377\n",
      "  0.80204307 0.8059688  0.72302738 0.85992729 0.76778892 0.86866686\n",
      "  0.7849911  0.74455222 0.71045084 0.74751146 0.75303978 0.88231417\n",
      "  0.70699699 0.81767636 0.7186858 ]\n",
      " [1.1299159  1.73678241 1.70108861 1.63209268 1.24573913 1.17166174\n",
      "  1.17693454 1.6663298  1.13029804 1.24124777 1.12768978 1.16812888\n",
      "  1.21634987 1.13392317 1.23887105 1.24017098 1.19215182 1.34132569\n",
      "  1.30165184 1.19149742 1.20026225 1.28672459 1.69238527 1.22516326\n",
      "  1.18699503 1.2412282  1.21361079 1.26812676 1.22683284 1.17583033\n",
      "  1.19362235 1.35699255 1.21487917 1.2011862  1.25260128 1.69570488\n",
      "  4.42797426 1.12767075 1.17668924 1.63724037 3.07246623 1.14539765\n",
      "  1.26922685 1.70900224 1.25625654 1.19620242 1.1986915  1.73981801\n",
      "  1.28557341 1.24561315 1.63740977 1.2527631  1.687533   1.1186095\n",
      "  1.25932486 1.19376139 1.21549145 1.09958827 1.69627686 1.63400573\n",
      "  1.21757517 2.20850807 1.23833306 1.25239533 1.19799035 1.0949596\n",
      "  1.33692107 2.09364188 1.30451503 1.17003591 1.28622407 1.19110364\n",
      "  1.19129703 1.35972852 1.16049861 1.20988364 1.25763215 1.19630738\n",
      "  2.14870101 1.21893029 1.19022771 1.13023978 1.15592638 1.2631164\n",
      "  1.19764651 1.20991235 1.28665997 1.20537521 1.22217857 1.22113286\n",
      "  1.22059467 1.24019187 1.17945498]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jianghaitao1\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\jianghaitao1\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# LDA主题模型 适用范围：长本文特征\n",
    "import pandas as pd\n",
    "import jieba\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "article_list = [\n",
    "    '沙瑞金赞叹易学习的胸怀，是金山的百姓有福，可是这件事对李达康的触动很大。易学习又回忆起他们三人分开的前一晚，大家一起喝酒话别，易学习被降职到道口县当县长，王大路下海经商，李达康连连赔礼道歉，觉得对不起大家，他最对不起的是王大路，就和易学习一起给王大路凑了5万块钱，王大路自己东挪西撮了5万块，开始下海经商。没想到后来王大路竟然做得风生水起。沙瑞金觉得他们三人，在困难时期还能以沫相助，很不容易。',\n",
    "    '沙瑞金向毛娅打听他们家在京州的别墅，毛娅笑着说，王大路事业有成之后，要给欧阳菁和她公司的股权，她们没有要，王大路就在京州帝豪园买了三套别墅，可是李达康和易学习都不要，这些房子都在王大路的名下，欧阳菁好像去住过，毛娅不想去，她觉得房子太大很浪费，自己家住得就很踏实。',\n",
    "    '347年（永和三年）三月，桓温兵至彭模（今四川彭山东南），留下参军周楚、孙盛看守辎重，自己亲率步兵直攻成都。同月，成汉将领李福袭击彭模，结果被孙盛等人击退；而桓温三战三胜，一直逼近成都。',\n",
    "]\n",
    "df = pd.DataFrame({'文章': article_list})\n",
    "df['文章分词'] = df['文章'].apply(lambda x: ' '.join(jieba.cut(x)))\n",
    "# print(df['文章分词'])\n",
    "# 去停用词\n",
    "stopwords_filename = '../../dataset/Stopwords-master/HIT_stopwords.txt'\n",
    "def get_stopwords_list(filepath):\n",
    "    stopwords = [line.strip() for line in open(filepath,'r',encoding='utf-8').readlines()]\n",
    "    return stopwords\n",
    "stopwords_list = get_stopwords_list(stopwords_filename)\n",
    "# 把词转化为词频向量，注意由于LDA是基于词频统计的，因此一般不用TF-IDF来做文档特征\n",
    "cnt_vector = CountVectorizer(stop_words=stopwords_list)\n",
    "cnt_tf = cnt_vector.fit_transform(df['文章分词'])\n",
    "print(cnt_tf)\n",
    "# LDA建模\n",
    "lda = LatentDirichletAllocation(n_topics=2,learning_offset=50.,random_state=0)\n",
    "docres = lda.fit_transform(cnt_tf)\n",
    "# 文档主题的分布\n",
    "print(docres)\n",
    "# 词列表\n",
    "print(cnt_vector.get_feature_names())\n",
    "# 主题和词的分布\n",
    "print(lda.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 词向量 适用范围：所有文本特征\n",
    "from mitie import total_word_feature_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
