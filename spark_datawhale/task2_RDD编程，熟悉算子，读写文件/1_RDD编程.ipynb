{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import findspark\n",
    "# # 可在环境变量中进行设置，即PATH中加入如下地址\n",
    "# findspark.init(\"F:/spark-2.4.3-bin-hadoop2.7\")\n",
    "# from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "# from pyspark import SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建sc\n",
    "sc = SparkContext('local','Simple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从文件系统中加载数据创建RDD\n",
    "fileRDD = sc.textFile('../../../dataset/word.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 通过并行集合（数组）常见RDD,可以调用SparkContext的parallelize方法，在Driver中一个已经存在的集合(数组)上创建\n",
    "nums = [1,2,3,4,5]\n",
    "rdd = sc.parallelize(nums)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 转换操作\n",
    "每一次转换操作都会产生不同的RDD，供给下一个转换使用，整个转换过程只是记录了转换的轨迹，并不会发生真正的计算，只有遇到行到操作时，才会发生真正的计算。<br>\n",
    "常见的转换操作（Transformation API）：\n",
    "* filter(func)：筛选出满足函数func的元素，并返回一个新的数据集\n",
    "* map(func)：将每个元素传递到函数func中，并将结果返回为一个新的数据集\n",
    "* flatMap(func)：与map()相似，但每个输入元素都可以映射到0或多个输出结果\n",
    "* groupByKey()：应用于(K,V)键值对的数据集时，返回一个新的(K, Iterable)形式的数据集\n",
    "* reduceByKey(func)：应用于(K,V)键值对的数据集时，返回一个新的(K, V)形式的数据集，其中的每个值是将每个key传递到函数func中进行聚合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 行动操作\n",
    "行动操作是真正触发计算的地方。Spark程序执行到行动操作时，才会执行真正的计算，从文件中加载数据，完成一次又一次转换操作，最终，完成行动操作得到结果。<br>\n",
    "下面列出一些常见的行动操作（Action API）：\n",
    "* count() 返回数据集中的元素个数\n",
    "* collect() 以数组的形式返回数据集中的所有元素\n",
    "* first() 返回数据集中的第一个元素\n",
    "* take(n) 以数组的形式返回数据集中的前n个元素\n",
    "* reduce(func) 通过函数func（输入两个参数并返回一个值）聚合数据集中的元素\n",
    "* foreach(func) 将数据集中的每个元素传递到函数func中运行*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 惰性机制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = sc.textFile('../../../dataset/word.txt')\n",
    "lineLengths = lines.map(lambda s : len(s))\n",
    "totalLength = lineLengths.reduce(lambda a ,b : a + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 持久化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "list_tmp = [\"Hadoop\",\"Spark\",\"Hive\"]\n",
    "rdd = sc.parallelize(list_tmp)\n",
    "print(rdd.count()) #行动操作，触发一次真正从头到尾的计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hadoop,Spark,Hive\n"
     ]
    }
   ],
   "source": [
    "print(','.join(rdd.collect())) #行动操作，触发一次真正从头到尾的计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[6] at parallelize at PythonRDD.scala:195"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.cache() #会调用persist(MEMORY_ONLY),但是，语句执行到这里，并不会缓存rdd,这是rdd还没有被计算生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(rdd.count()) # 第一次行到操作，触发一次真正从头到尾的计算，这时才会执行上面的rdd.cache(),吧这个rdd放到缓存中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hadoop,Spark,Hive\n"
     ]
    }
   ],
   "source": [
    "print(','.join(rdd.collect())) #行动操作，不需要触发从头到尾的计算，只需要重复使用上面缓存中的rdd\n",
    "# 最后使用unpersist()方法手动地把持久化的RDD从缓存中移除。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 分区 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = [1,2,3,4,5]\n",
    "rdd = sc.parallelize(array,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
